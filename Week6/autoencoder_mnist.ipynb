{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/knazari/Advanced_Robotic_2022/blob/main/Week6/autoencoder_mnist.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9FaSl401sppZ"
      },
      "source": [
        "## **Introduction**\n",
        "\n",
        "In this workshop, we are going to train a very basic autoencoder. An autoencoder is an unsupervised machine learning technique where you train the model on input data without any labels/ground-truth (thats why its under unsupervised branch) in the hopes that the trained model will capture the important co-relations between the inputted data dimensions. \n",
        "\n",
        "The goal of training such a network is to find the nonlinear relation among multiple dimensions of input dataset. Some of the applications of the autoencoders are dimentionality reduction, data compression, anomaly detection and many more. \n",
        "\n",
        "An autoencoders contains three parts/layers: \n",
        "\n",
        "1) An encoding layer called as 'Encoder'. \n",
        "\n",
        "2) A decoding layer called as 'Decoder'.\n",
        "\n",
        "3) A bottleneck-layer where your input data is transformed/reduced/compressed into a simpler representation. \n",
        "\n",
        "\n",
        "![Autoencoder](https://sci2lab.github.io/ml_tutorial/images/autoen_architecture.png)\n",
        "\n",
        "### **Similar Techniques:**\n",
        "Similar to autoencoders, there are other dimentionality reduction algorithms/techniques such as [Principle Component Analysis (PCA)](https://en.wikipedia.org/wiki/Principal_component_analysis#:~:text=Principal%20component%20analysis%20(PCA)%20is,components%20and%20ignoring%20the%20rest.). Major differences between PCA and autoencoders are that autoencoders able to capture the nonlinearity of the input data as opposed to PCA which works well with linear relationship mostly.\n",
        "\n",
        "### **Why Autoencoders:**\n",
        "\n",
        "One of the reason to use autoencoders is to capture the latent space representation of a trained autoencoders and then use this representation for further training of different models such as a Convolutional Nueral Network (CNN) or a Fully Connected Network (FCN). \n",
        "\n",
        "The motivation is that if we train the CNN or FCN directly on our original datasets (normally in the form of images), these data images may contain noise and other useless information are we are not interested in training for in CNN and FCN. In contrast, using the latent space representation of our original image datasets to train a CNN or FCN may result in accuracy as well as it will train our CNN or FCN models faster. \n",
        "\n",
        "Of course there are many other reasons people use autoencoders. \n",
        "\n",
        "\n",
        "### **Training Code:** \n",
        "\n",
        "In this workshop, We will use ```keras``` library to train an autoencoder model on the MNIST datasets. \n",
        "### **Task:**\n",
        "Your task will be to train a similar network on the breast phantom RGB images. The dataset for the RGB images are available [here](https://drive.google.com/drive/folders/1sSwIe5eejJDgYfWnbR3Qry-3qiGrWDWF). You can download the ```rtp-rgbd``` dataset and only use color images from this dataset for the autoencoder training. \n",
        ". "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pewX8_F4smJS",
        "outputId": "9dbeeaa2-5d53-4207-877b-dbe4bc181734"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "# these two lines allow access from colab file to your google drive \n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-GQ0hzpjx-KL"
      },
      "outputs": [],
      "source": [
        "# os is a small module use to set env. variables and other things\n",
        "import os \n",
        "import time\n",
        "# NumPy is an amazing numerical calculation library in Python.\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "# TensorFlow is GPU/CPU-based ML library from Google (opensource, so nice of them) \n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers import Input, Dense\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.optimizers import Adam "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qI4X8qQwy5UC"
      },
      "outputs": [],
      "source": [
        "# Lets seed the psuedo-random-number-generator in NumPy and TensorFlow.\n",
        "# If you are interested to know why we need to seed the psuedo-random-number-generator, \n",
        "# first you should ask why it is called 'psuedo-random-number-generator' and not \n",
        "# 'raodom-number-generator'. Long story short, COMPUTERS ARE DUMB! \n",
        "# Ref: https://www.youtube.com/watch?v=SxP30euw3-0\n",
        "np.random.seed(int(time.time()))\n",
        "tf.random.set_seed(time.time() * 1000)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the MNIST dataset\n",
        "dataset = tf.keras.datasets.mnist\n",
        "(X_train, Y_train) , (X_test, Y_test) = dataset.load_data()\n",
        "\n",
        "# Note that we don't use Y_train and Y_test (which are labels \n",
        "# or ground truth) in autoencoder. \n",
        "\n",
        "# Lets see what the dataset looks like \n",
        "# N is the number of images, \n",
        "# W, H is the with and height of each image\n",
        "print (f\"Shape of the train dataset images in (N, W, H) is {X_train.shape}\")\n",
        "print (f\"Shape of the test dataset images in (N, W, H) is {X_test.shape}\")\n",
        "\n",
        "# Lets take a look at a single WxH image. \n",
        "print (f\"Shape of a single image in (W, H) is {X_train[0].shape}\")\n",
        "print (f\"Type of a single pixel in one image is {X_train[0].dtype}\")\n",
        "\n",
        "# -------------------\n",
        "#    PREPROCESSING \n",
        "# -------------------\n",
        "# Normalize the data from 0 - 255 to 0 - 1.\n",
        "X_train = X_train/255\n",
        "X_test = X_test/255\n",
        "\n",
        "# Save image height, width and channel \n",
        "# into some variables (used later).\n",
        "imgW, imgH = X_train[0].shape\n",
        "imgC = 1\n",
        "\n",
        "X_train = np.reshape(X_train, (-1, imgH * imgW * imgC ))\n",
        "X_test = np.reshape( X_test, (-1, imgH * imgW * imgC ) )\n",
        "print (f\"Train dataset images after flattening in (N, D) is {X_train.shape} where D = H * W * C\")\n",
        "print (f\"Test dataset images after flattening in (N, D) is {X_test.shape} where D = H * W * C\")"
      ],
      "metadata": {
        "id": "yi28cCZcCaRU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2035a78b-e6d9-4143-dae2-b4e7188ba76f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
            "11493376/11490434 [==============================] - 0s 0us/step\n",
            "11501568/11490434 [==============================] - 0s 0us/step\n",
            "Shape of the train dataset images in (N, W, H) is (60000, 28, 28)\n",
            "Shape of the test dataset images in (N, W, H) is (10000, 28, 28)\n",
            "Shape of a single image in (W, H) is (28, 28)\n",
            "Type of a single pixel in one image is uint8\n",
            "Train dataset images after flattening in (N, D) is (60000, 784) where D = H * W * C\n",
            "Test dataset images after flattening in (N, D) is (10000, 784) where D = H * W * C\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "ypMQRtOrLPBW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YDa-d7eVG5uD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "de2b0eab-4908-4ffa-fb20-d95c57cb587b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"autoencoder_mnist_workshop\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " input_layer (InputLayer)    [(None, 784)]             0         \n",
            "                                                                 \n",
            " bottleneck_layer (Dense)    (None, 64)                50240     \n",
            "                                                                 \n",
            " output_layer (Dense)        (None, 784)               50960     \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 101,200\n",
            "Trainable params: 101,200\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "# Build an autoencoder model structure. \n",
        "LATENT_SPACE_DIM = 64\n",
        "layer_input = Input( shape = (imgH * imgW * imgC ), name = \"input_layer\")\n",
        "layer_bottleneck = Dense( LATENT_SPACE_DIM, activation = \"relu\" , name = \"bottleneck_layer\")(layer_input) \n",
        "layer_output = Dense( imgH*imgW*imgC, activation = \"sigmoid\" , name = \"output_layer\")(layer_bottleneck)\n",
        "model_autoencoder = Model( layer_input, layer_output, name = \"autoencoder_mnist_workshop\" )\n",
        "model_autoencoder.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0bP1j5XeI32T",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "35132a4a-16f4-49a7-cedd-12fffa7077ea"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "235/235 [==============================] - 4s 12ms/step - loss: 0.2439 - val_loss: 0.1628\n",
            "Epoch 2/5\n",
            "235/235 [==============================] - 3s 11ms/step - loss: 0.1436 - val_loss: 0.1249\n",
            "Epoch 3/5\n",
            "235/235 [==============================] - 3s 11ms/step - loss: 0.1166 - val_loss: 0.1071\n",
            "Epoch 4/5\n",
            "235/235 [==============================] - 3s 11ms/step - loss: 0.1026 - val_loss: 0.0964\n",
            "Epoch 5/5\n",
            "235/235 [==============================] - 3s 11ms/step - loss: 0.0938 - val_loss: 0.0893\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7fec2d146510>"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ],
      "source": [
        "# Fit the model on the data \n",
        "# Note that we use Adam optimzer to \n",
        "# change the weights of the network. Adam \n",
        "# is a fancier version of the gradient descent. \n",
        "# Ref: https://arxiv.org/abs/1412.6980\n",
        "\n",
        "LEARNING_RATE = 1e-3\n",
        "NUM_EPOCHS = 5\n",
        "BATCH_SIZE = 256\n",
        "model_autoencoder.compile(optimizer = Adam(LEARNING_RATE), loss = \"binary_crossentropy\")\n",
        "model_autoencoder.fit( X_train, X_train, epochs = NUM_EPOCHS, batch_size = BATCH_SIZE, validation_data = (X_test, X_test) , shuffle=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JXaWC_FDLaCs",
        "outputId": "4818dbbc-ee01-45ee-a80a-7542aaf5b43c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/Colab-Notebooks/AdvanceRoboticsWorkshop/autoencoders/model/rtp-rgbd/model_on_mnist/assets\n"
          ]
        }
      ],
      "source": [
        "# Save your train model (for later use). \n",
        "MODEL_SAVED_PATH = \"/content/drive/MyDrive/Colab-Notebooks/AdvanceRoboticsWorkshop/autoencoders/model/rtp-rgbd/model_on_mnist/\"\n",
        "\n",
        "# make sure that the directory exists. \n",
        "# if not, then create one.\n",
        "if not os.path.exists(MODEL_SAVED_PATH):\n",
        "  os.mkdir(MODEL_SAVED_PATH)\n",
        "\n",
        "# save the model\n",
        "tf.keras.models.save_model( model_autoencoder, MODEL_SAVED_PATH , overwrite = True )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1PpQOr8mMEGE"
      },
      "outputs": [],
      "source": [
        "# Load the model from drive \n",
        "loaded_autoencoder_model = tf.keras.models.load_model(MODEL_SAVED_PATH)\n",
        "loaded_autoencoder_model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZVSDkv-qMhm2"
      },
      "outputs": [],
      "source": [
        "# Do the predictions using the loaded autoencoder model. \n",
        "# Please note that the test input should be in the format \n",
        "# of (N, M) where N is number of test images and M is \n",
        "# each image HxWxC and normalized between 0 - 1.\n",
        "predicted_images = loaded_autoencoder_model.predict( X_test )"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "input_random_image = np.random.rand( 10, imgC*imgH*imgW )\n",
        "predicted_random_image = loaded_autoencoder_model.predict( input_random_image ) \n",
        "\n",
        "input_random_image = input_random_image * 255\n",
        "predicted_random_image = predicted_random_image * 255\n",
        "\n",
        "input_random_image = np.reshape( input_random_image, (-1, imgW, imgH) )\n",
        "predicted_random_image = np.reshape( predicted_random_image, (-1, imgW, imgH) )\n",
        "\n",
        "\n",
        "fig = plt.figure()\n",
        "fig.suptitle(\"Results\" , fontsize=16)\n",
        "\n",
        "for i in range(10):\n",
        "  ax = plt.subplot(\"121\")\n",
        "  ax.set_title(\"Original\")\n",
        "  plt.imshow( input_random_image[i] )\n",
        "\n",
        "  ax = plt.subplot(\"122\")\n",
        "  ax.set_title(\"Predicted\")\n",
        "  plt.imshow( predicted_random_image[i] )\n",
        "  plt.show()"
      ],
      "metadata": {
        "id": "wt-yPE-kr4yI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2\n",
        "img = cv2.imread(\"/content/drive/MyDrive/Colab-Notebooks/AdvanceRoboticsWorkshop/autoencoders/7.png\", 0)\n",
        "\n",
        "img = 255 - img\n",
        "\n",
        "img_normalize = img.copy()\n",
        "img_normalize = img_normalize / 255\n",
        "img_normalize = np.reshape( img_normalize , (1, imgC* imgH * imgW) )\n",
        "\n",
        "\n",
        "predicted_random_image = loaded_autoencoder_model.predict( img_normalize ) \n",
        "predicted_random_image = predicted_random_image * 255\n",
        "predicted_random_image = np.reshape( predicted_random_image, (-1, imgW, imgH) )\n",
        "\n",
        "img_arr = np.reshape( img, (1, imgW, imgH) )\n",
        "\n",
        "fig = plt.figure()\n",
        "fig.suptitle(\"Results\" , fontsize=16)\n",
        "\n",
        "for i in range(1):\n",
        "  ax = plt.subplot(\"121\")\n",
        "  ax.set_title(\"Original\")\n",
        "  plt.imshow( img_arr[i] )\n",
        "\n",
        "  ax = plt.subplot(\"122\")\n",
        "  ax.set_title(\"Predicted\")\n",
        "  plt.imshow( predicted_random_image[i] )\n",
        "  plt.show()"
      ],
      "metadata": {
        "id": "sZ_KcEbFuNkJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x0-ZadPuMrnC"
      },
      "outputs": [],
      "source": [
        "# Fix the normalization (from 0 - 1 to 0 - 255).\n",
        "# Fix the shape from (N, M) -> (N, H, W, C)\n",
        "predicted_images = np.reshape( predicted_images * 255, (-1, imgH, imgW ) )\n",
        "predicted_images = predicted_images.astype(np.uint8)\n",
        "\n",
        "text_X_ = np.reshape( X_test * 255, (-1, imgH, imgW ) )\n",
        "text_X_ = text_X_.astype(np.uint8)\n",
        "\n",
        "\n",
        "fig = plt.figure()\n",
        "fig.suptitle(\"Results\" , fontsize=16)\n",
        "\n",
        "for i in range(5):\n",
        "  ax = plt.subplot(\"121\")\n",
        "  ax.set_title(\"Original\")\n",
        "  plt.imshow( text_X_[i] )\n",
        "\n",
        "  ax = plt.subplot(\"122\")\n",
        "  ax.set_title(\"Predicted\")\n",
        "  plt.imshow( predicted_images[i] )\n",
        "  plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# We access the bottleneck layer output.  \n",
        "input_to_bottleneck_model = tf.keras.Model(loaded_autoencoder_model.get_layer(\"input_layer\").input, loaded_autoencoder_model.get_layer(\"bottleneck_layer\").output)\n",
        "input_to_bottleneck_model.summary()\n",
        "\n",
        "# This means we can give the trained model any \n",
        "# input image and look at the bottle neck layer output. \n",
        "encoded_images = input_to_bottleneck_model.predict(X_test)\n",
        "\n",
        "# Fix the normalization (from 0 - 1 to 0 - 255).\n",
        "# Fix the size i.e. bottleneck layer has 64 dimension \n",
        "# we resize it encoded images to 8x8 \n",
        "encoded_images = np.reshape( encoded_images * 255, (-1, 8, 8 ) )\n",
        "encoded_images = encoded_images.astype(np.uint8)\n",
        "\n",
        "text_X_ = np.reshape( X_test * 255, (-1, imgH, imgW ) )\n",
        "text_X_ = text_X_.astype(np.uint8)\n",
        "\n",
        "\n",
        "fig = plt.figure()\n",
        "fig.suptitle(\"Results\" , fontsize=16)\n",
        "\n",
        "for i in range(15):\n",
        "  ax = plt.subplot(\"121\")\n",
        "  ax.set_title(\"Original\")\n",
        "  plt.imshow( text_X_[i] )\n",
        "\n",
        "  ax = plt.subplot(\"122\")\n",
        "  ax.set_title(\"Predicted\")\n",
        "  plt.imshow( encoded_images[i] )\n",
        "  plt.show()\n"
      ],
      "metadata": {
        "id": "zJhn_xpgLzh3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Out of curiosity, lets see what happens \n",
        "# if we make a model that takes in (1, 64) as input image \n",
        "# as encoded image and see what is the trained model's \n",
        "# decoder output. \n",
        "bottleneck_to_output = tf.keras.Model(loaded_autoencoder_model.get_layer(\"bottleneck_layer\").output, loaded_autoencoder_model.get_layer(\"output_layer\").output)\n",
        "bottleneck_to_output.summary()\n",
        "\n",
        "# We have to give an input of (1, 64) for 1 image prediction. \n",
        "# Lets use a psueudo number generator to create an array \n",
        "# of (1, 64) and pass it to bottleneck_layer as encoded \n",
        "# input image. \n",
        "encoded_images_to_predict = 50\n",
        "encoded_image_size = LATENT_SPACE_DIM\n",
        "random_X_test = np.random.rand( encoded_images_to_predict, encoded_image_size ) \n",
        "\n",
        "predicted_output_images = bottleneck_to_output.predict( random_X_test )\n",
        "\n",
        "# Fix the normalization (from 0 - 1 to 0 - 255).\n",
        "# Fix the shape from (N, M) -> (N, H, W, C)\n",
        "predicted_output_images = np.reshape( predicted_output_images * 255, (-1, imgH, imgW ) )\n",
        "predicted_output_images = predicted_output_images.astype(np.uint8)\n",
        "\n",
        "\n",
        "# Fix the normalization (from 0 - 1 to 0 - 255).\n",
        "# Fix the size i.e. bottleneck layer has 64 dimension \n",
        "# we resize it encoded images to 8x8 \n",
        "random_X_test = np.reshape( random_X_test * 255, (-1, 8, 8 ) )\n",
        "random_X_test = random_X_test.astype(np.uint8)\n",
        "\n",
        "\n",
        "fig = plt.figure()\n",
        "fig.suptitle(\"Results\" , fontsize=16)\n",
        "\n",
        "for i in range(encoded_images_to_predict):\n",
        "  ax = plt.subplot(\"121\")\n",
        "  ax.set_title(\"Original\")\n",
        "  plt.imshow( random_X_test[i] )\n",
        "\n",
        "  ax = plt.subplot(\"122\")\n",
        "  ax.set_title(\"Predicted\")\n",
        "  plt.imshow( predicted_output_images[i] )\n",
        "  plt.show()\n"
      ],
      "metadata": {
        "id": "Duhg83fOOrDu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "ODu7NU3fPywB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Summary**\n",
        "\n",
        "In this workshop, we trained an autoencoder model on MNIST digits dataset, saved the mode and looked at its prediction from three different layers i.e. \n",
        "\n",
        "1) Input to Output \n",
        "\n",
        "2) Input to Bottleneck \n",
        "\n",
        "3) Bottleneck to Output\n",
        "\n",
        "We noticed that if we give random encded images, they do NOT reconstruct to number which says something about our trained model i.e. it actually trained and encoded the dataset into some latent space. We can use this reduced latent space representation to do other cool things such as use it in a CNN model to train another model which we will try next time (probablly). "
      ],
      "metadata": {
        "id": "hK1Tj2GGQuxe"
      }
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "autoencoder_mnist.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}